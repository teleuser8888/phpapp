
L’informatica molto spesso riprende dei concetti già utilizzati e li riutilizza in modo più o meno nuovo, con logiche tutto sommato simili. C’è un continuo andirivieni di soluzioni che sono più o meno adatte e parte del nostro compito è riuscire a interfacciarci con le soluzioni più adeguate rispetto alle necessità del momento. Un codice, qualunque esso sia, si caratterizza per avere uno standard CRUD, ovvero creare, leggere, aggiornare e eliminare dati. Questi creano come una sorta di ricetta che permette, a partire da un input, di ottenere un output, che possa poi a sua volta essere un input o risultato finale; questa attività può essere completamente automatica e ripetibile all'infinito o meno. 

# Una breve storia

Negli anni 50 la soluzione di computing più diffusa per le aziende era il mainframe, e si operava comunicando grazie a schede perforate. 
La programmazione talvolta avveniva in base a interruttori chiusi o aperti, ovvero delle valvole che controllavano una serie di impostazioni, gestendo i circuiti in modo pressoché completamente manuale. 
Era un potente (per l’epoca) sistema centralizzato. Il tempo di consumo di CPU veniva notarizzato e di conseguenza i costi per l’utente finale erano calcolati in base a questo. Erano in grado di fare calcoli e di mantenere i dati di volta in volta. 

Ancora oggi, il multitasking avviene solo apparentemente. Ad ogni operazione viene affidato un tempo per poter essere eseguita, ma se ne fa una alla volta. Ovvero, si possono fare conti uno alla volta: quello che accade è che un thread si occupa di una operazione in particolare e si hanno delle code. Da questo punto fondamentale possono anche verificarsi quelle che sono note come [[Race Condition]], un problema sia per la sicurezza che per l'affidabilità dei sistemi. Nei Mainframe al giorno d’oggi (come in realtà anche allora) avvengono dei controlli di integrità che permettono di eseguire in parallelo le attività e monitorarne gli errori. 

I CED erano fondamentali: luoghi in cui sono effettuati elaborazioni ricorrenti. I centri elaborazione dati hanno delle funzioni definite, in cui in modo veloce ci si occupa di una serie di operazioni per un qualche tipo di attività. 

AS 400, altro sistema che ha fatto la storia. Questi sistemi erano pensati per centralizzare le operazioni: si ha la struttura di terminali collegati a un centro che si occupa delle operazioni, in collegamento punto punto. In effetti si hanno tanti computer all’interno di una singola macchina. Solamente la GUI la vedo al terminale, mentre le operazioni effettivamente avvengono su un computer soltanto. Si avevano porte seriali, ognuna collegata a un terminale, e all’interno predisposta per essere considerata una macchina a se. Ad oggi questo stesso principio lo si ritrova nei sistemi Docker e non soltanto. Il cloud computing al giorno d’oggi fa si che si usano dei servizi che non prevedono che l’elaborazione venga fatta sul terminale. 

Internet all’inizio non aveva quelle funzioni di repository che ha ora, funzionava da idsb 

All’inizio le CDN erano effettivamente delle strutture dedicate per le aziende, in cui si poteva far passare chiamate VOIP come pure altri dati e simili. Ancora oggi esistono CDN di vario tipo, ma non necessariamente su linee dedicate. 

Arpanet, le prime sperimentazioni si fecero tra 4 università. Al momento si stavano diffondendo i primi personal computer, tra l’altro. La programmazione si faceva a spaghetti, ovvero GO-TO. E Python al giorno d’oggi ha ripreso certi concetti per averne la facilità d’uso. Per esempio i cicli while che non sono definiti fino a, mentre non si deve dare una specifica definita. La potenza di calcolo poteva iniziare a essere distribuita e internet nacque in un momento utilissimo. 

All’inizio ogni utente aveva un collegamento punto punto con un modem, che poi si interfacciava poi con BBS che servivano da cache di contenuti. Questo faceva si che servissero enormi numeri di connessioni telefoniche. Per questo spesso non si poteva usare telefono e nel contempo navigare: per risparmiare si faceva in modo di usare lo stesso collegamento. E le linee potevano finire, per cui si andava pagando a volume, e in base alle zone si avevano anche tempi diversi. 

A quel punto si cominciarono a programmare sistemi distribuiti, in cui risorse devono e ssere condivise, quindi dati memorizzati. SI programmavano con sistemi CORBA, COM e DCOM, ovvero logiche di comunicazione. Le applicazioni dovevano comunicare avendo presenti degli archivi specifici. Da qui gli Extraned, overlay networks, grid e ubiquitous networks. 

SI passò da un tipo di programmazione sequenziale, in cui ogni operazione avviene in un dato momento specifico. Proprio come si fa con degli script. Al contrario la stragrande maggioranza dei sistemi di oggi si basa sugli eventi, ovvero un’azione e una reazione. Questo era fondamentale perchè una macchina potesse aspettare un’altra. Sussiste uno strato software che codifica gli eventi e li può, in base a com’è programmato, agire. Ci sono varie classi disponibili che danno ppssibiltà di definire comportamenti. Questa stessa logica è  alla base dei sistemi web odierni. 

## VM

La virtualizzazione è stata un’ottima soluzione per ottenere su macchine molto potenti più servizi. Ogni macchina virtuale è concorrente e complessa: crea più strati e interagisce col kernel. Il kernel è quel componenti che fa interagire la macchina con il software. Un sistema operativo intelligente e robusto non permette di far passare in questo. Nelle VM si ha un hypervisor che finge di essere il kernel. E questo può darci modo di dare alle macchine accesso alle varie interfaccia fisiche. Questo sistema fa si che lo stesso hardware possa avere più componenti. Utile anche a fare prove. Così si parcellizano le risorse. Anche in questo caso la comunicano e avvien attraverso le reti, per cui ogni macchina ha indirizzo ip, diverso e gestito o dalla VM o da un sistema come il router. SI possono avere più casi, o che si abbia indirizzi con la stessa classe, oppure che questi siano schermati del tutto o che possano comunicare tutti con l’esterno. Un servizio potrebbe essere il database e l’altro le immagini.

## Cloud 

Nei primi 2000 l’idea di cloud computing comincia a emergere, in cui la potenza di calcolo viene da un elemento centrale. Ora però i sistemi però non si creano come monolitici, ma scomposti in tanti microservizi che possano interagire e ciascuno dei quali soddisfa una singola esigenza. Emergono sistemi SOA e API - Restful. Delete, Update, Patch, Get sono istruzioni utili a farli funzionare. Si hanno quindi delle macchine atomiche che danno una singola azione. Kasmiss, NGINX, APACHE sono tutte componenti poi utili. Postgres, MYSQL utili per i database. Si torna ad avere piccole macchine potenti in grado di agire e che hanno però logiche di programmazione più avanzate. 

Attualmente si sta migrando dall’esterno verso l’interno, a fronte di una pervasità della rete che permette di legare le varie applicazioni. 

A fronte dal passaggio da sistemi informativi monolitici a casistiche cloud, si è avuto un cambio di paradigma. Prima macchine diverse inviavano i record a un punto fondamentale, mentre ora tutto può avvenire ovunque. 

Le applicazioni di desktop remoto permettono di utilizzare il proprio come un terminale stupido. Possono operare sulla stessa rete con una vpn. Mentre sistemi come Anydesk hanno un loro proprio modo di scavalcare la rete locale e permettere l’accesso da qualsiasi punto della rete grazie a delle credenziali. Questo perché Anydesk manda una chiamata verso l’esterno, ovvero i server di Anydesk, e comunicano loro a livello di ip necessari e simili, perciò c’è una vulnerabilità in più a fronte di maggiore comodità.  Ho in ogni caso un flusso video che può venire compresso con diffenti codec e un flusso di input che inserisco. 
Dal desktop remoto si è piano piano tornati a computer centralizzati che possano elaborare e che permettano di replicare la parte grafica in maniera locale. Da qui sistemi come visual desk o simili. Localmente il software gira ma si interfaccia con dei connettori a un serve esterno che fa elebaoraizoni e poi il dosatore locale procede a raffigurare i dati. Non ho un flusso video e input, ma un software che fa solo interpretazione di quello che viene mandato. 

Il cloud ha permesso l’evoluzione di sistemi pay-per-use. Questo perché pur basandosi su hardware con logiche simili di funzionamento, ci si è potuti slegare dall’hardware. Questo grazie anche a sistemi che poi sono in grado di lavorare con richieste API che sono contabilizzate.

Autenticazione e autorizzazione. La gestione di un token è fondamentale rispetto a sistemi stateless o meno. 

Finanche i supercomputer ed edge computer che fanno conti valorizzati parziali e poi li vanno a utilizzare insieme.
Edge computing: calcoli pre digeriti che sono poi inviati. 
Reti neurali: sistemi esperti e reti neurali. Le seconde partono da un apprendimento si desidera creare dei pattern. 

## Container 

Il docker comunque ha un modo di orchestrare il tutto ma può interagire direttamente col kernel. Io posso muovermi in modo additivante, e gli strati condivisi sono sempre gli stessi, migliorando la velocità. E nel caso dei docker si ha che se questo organizzatore si spegne si cancella il tutto, per cui si ha che devono avere sempre accesso a dei volumi. La velocità di avvio acquista fa si che si possa avere sistemi in on demand e non sempre in attesa oppure in caso di grande traffico l’organizzatore può creare più macchine gemelle e dar loro in pasto le operazioni in modo bilanciato. La resilienza la ottengo proprio da questo tipo di interazione. 

Si tratta di una soluzione che permette di risparmiare risorse, avendo virtualizzato dal kernel in sù. E questo ha degli effetti notevoli: si semplifica e rende agile il deploy, la scalabilità, mantenendo interoperabilità e 
# Un nuovo capitolo della stessa storia? Webapp e Cloud

Il passaggio successivo è non aver è un software da aggiornare mantenere, ma far si che i dati siano incapsulati in codice che possa contenere in se elementi grafici: da qui html e css, bootstrap e simili per creare web app. Si ha quindi un software che filtra e che interpreta dei metalinguaggi e li mostra a video ma l’elaborazione non è in locale. In ambito web il browser prende in input un flusso di dati di cui parte sono istruzioni, rappresentazioni e dati veri e propri. Per superare la staticità di HTML si usa JavaScript, anch’esso un metalinguaggio, che è dedicato ad operazioni di scripting. Permette di mettere in campo delle azioni che html non sarebbe in grado di fare, iniettando del codice javascript nelle pagine html. Si è tornati a tecnologie che bilanciano il carico avendo delle azioni sul server e altre sul client, minimizzando il flusso dati. 

Si è oltrepassati le macchine a stati verso quelli a stati. Una macchina a stati aspetta un input pronta, in maniera iterativa e progressiva. Le macchine ad eventi sono una evoluzione che sfrutta un background condiviso: le azioni sono svolte a fronte di particolari input, come un click o simili. Il tutto è nato al passaggio alle macchine programmate in modo visuale. Pensiamo alla differenza tra operare da terminale o in modo visuale: sono usati gli stessi comandi ma con o senza visualità.

Un servizio di Apirest funziona a chiamate. I microservizi devono riceve delle risposte in JSON. Un errore comune è nella paginazione, che devono essere fatte secondo delle logiche prestabilite perché non vengano date risposte con un eccessivo numero di record. Lo standard apirest chiede di strutturare le risposte con un header che lo specifichi in modo più preciso. Seguirle garantisce con maggiore sicurezza l’interoperabilità. Vedremo anche come modificare un servizio web monolitico perchè inizi a funzionare per microservizi. 

I backend sono proprio costituiti da degli endpoint API, ognuno dei quali identificato da URI che danno singoli comandi. Ognuno di questi si occupa di una piccola azione per far progredire il programma. 

Ad oggi con AI e simili applicazioni, NVDIA sta facendo da padrone nel mercato: questo ha fatto si che sia diventata una compagnia determinante per l’economia. Questo perché si sta tornando a server che rendono disponibile la propria potenza di calcolo per l’IA. 

Al giorno d’oggi è spesso una grande opportunità andare ad affittare potenza di calcolo per applicazioni che hanno esigenze specifiche e temporanee. Proprio come ai tempi del mainframe, il paradigma del cloud riprende proprio i vantaggi di qualcosa di centralizzato. 
D’altronde le attività sul web ripercorrono proprio dei concetti della programmazione ad eventi. 

# Edge Routing cfr Cloud Computing

È da considerarsi una sorta di evoluzione, che va di pari passo con l’avvento dei big data: sensori che riescono a registrare in parallelo dati connessi tra loro e correlati, spesso in modo imprevisto. Un po’ come ascoltare le voci di diverse persone da più angoli, li si hanno dati simili e spesso sovrapposti ma da più fonti grazie alle quali poi è possibile analizzare la tendenza più che il dato preciso, il pattern. 

Questo determina anche una certa congestione dei dati. Per questo l’edge routing è interessante: si svolge una pre elaborazione. Questo permette di ridurre i dati che devono essere analizzati, oppure identificano gli eventi e permettono di cambiare modalità di memorizzazione nel caso di differente evento. 

L’edgerouting permette quindi di raccogliere dati significativi per allenare sistemi che siano in grado di riconoscere i pattern, creando un modello matematico rappresentativo del caso in interesse. 

Avendo creato il modello anche computer meno potenti sono in grado di eseguire il modello avendo il meccanismo già elaborato. Gli edge sono quindi in grado di eseguire riconoscimento basato su questi modelli. Il cloud è la disponibilità di azioni, spesso pre elaborate e in questo assomiglia all’edgerouting, perché mette un servizio a disposizione rispetto a dei dati che seguono un certo standard e che è poi in grado di agire in maniera utile. 

