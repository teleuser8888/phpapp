> Si tratta della soluzione che permette di orchestrare container in maniera automatizzata basandosi su dei parametri ben definiti, per sua natura basata su una programmazione dichiarativa.

È un orchestratore non di singoli container ma di gruppi definiti per container omogenei identificati con un POD. Questi hanno capacità determinate di CPU, RAM, rete e archiviazione. 
I POD possono essere scalati orizzontalmente con replicas set, con valori di massimo e minimo. Kubernetes li farà girare nel nodo in cui si ha la capacità di far girare quel POD con le capacità di farlo e non gli permetterà di fuoriuscire dai suoi limiti. 

Interessante è che il suo utilizzo permette di lavorare puntando al risultato piuttosto che alla funzione. È adatto a infrastrutture complesse e che devono reagire in modo autonomo a cambi repentini. Infatti, spesso fa il paio con una serie di elementi tipici di un sistema ad alta disponibilità, ad esempio con controlli di integrità e con la capacità di amministrare graduali aggiornamenti con un load balancing, in modo da non rendere mai completamente offline l'intera infrastruttura.

Tramite replicas set è in grado i far girare gli elementi su differenti nodi in base anche alla zonizzazione. 
	Nel caso del nostro progettino possiamo scalare in maniera verticale il DB, ovvero le capacità della macchina su cui gira, ma non è possibile farla funzionare in maniera orizzontale. Quindi un POD identifica il DB, mentre un altro POD conterrà BE e FE, che sono tendenzialmente accoppiati. 
Un POD può essere presente in un solo nodo e non oltre. Altrimenti sono utilizzati i replicaset. 

# Architettura

Kubernetes si basa su un modello distribuito composto di cluster di differenti nodi. In alcuni di questi sono in esecuzione un insieme di container che costituisce un’unità omogenea e replicabile per funzionalità esposte come risultato finale, definita POD. Questo insieme è quello che viene replicato per scalare in senso orizzontale. Per poter scalare in senso verticale è invece necessario scomodare risorse maggiori che possano ospitali nodi più “capienti”. 

Un nodo può essere sia una macchina fisica che una virtuale che mette a disposizione le proprie risorse. Se ne distinguono due tipi.
- Worker: riceve esclusivamente in pasto i POD, ospita componenti come *kubelet* e *kube-proxy*
- Control Pane o Nodo Master: un nodo evoluto che ha tre componenti integrate: 
	- API (strumento di interfaccia coi componenti), con *kube-apiserver* 
	- ETCD (gestione dei parametri di tutto K8) 
	- lo scheduler (che si occupa di gestire i POD seguendo un file di configurazione in base a specificità del nodo e dei POD in causa), con *kube-scheduler*, *kube-controllermanagement* e *cloud-controllermanager*

![[Sensini - Architettura Kubernetes con componenti.png]]

 > Nota: un controller è un oggetto che esegue un loop infinito per verificare delle condizioni; questo significa che è costantemente alla ricerca di un cambiamento dello stato del cluster e ne controlla lo stato reale con quello desiderato. Se c’è una differenza tra lo stato effettivo e quello desiderato, questo componente garantisce che la risorsa/oggetto Kubernetes si trovi nello stato desiderato. 
 
![[K8 Structure.png]]
## Una panoramica…

KubeCTL è un interprete di comando che accede all’APIServer presente sul nodo master per poter interagire con le azioni. Eseguendo le cose in modalità imperativa con il comando `kubectl run nome_del_pod immagine_container_da_incapsulare` ad esempio sulla console nel nodo master (ma non necessariamente) si invia all’APIServer richiesta di schedulare all’interno del nodo caratterizzato uno spazio necessario a far funzionare il POD di interesse. 

![[Kubernetes Cluster.svg]]

I kubelet sono gli elementi che fanno in modo che un set di POD girino all'interno del cluster. 

Il minimo di risorse da assegnare è il punto di partenza per lo schedulatore che permette di individuare il nodo dove il POD può essere lanciato. Inoltre, deve anche essere definito il limite massimo di potenza di calcolo e memoria al cui un POD può avere accesso. Questo perché altrimenti ci sarebbe il rischio di incorrere in errori, ad esempio con sovrascritture o errate attribuzioni di memoria. 

Che succede se il nodo master cade? Kubernetes ha capacità di **self-healing.** 
	Esistono delle regole inserite nel cluster che permettono a ciascun nodo di candidarsi a diventare master (liberandosi dei carichi che ha e distribuendoli diventando il manager degli altri). Ogni nodo ha un watchdog per cui se non si sente contattato da un master considera che manchi un master e che sia caduto e passa a candidarsi, e il compito degli altri nodi è accettare tale richiesta. Tutto ciò avviene in modo trasparente per il sistema dall'esterno. 

Proprio come per Traefik ad ogni componente del proprio kubernetes, quindi i services, replicaset, worker, dei label che permettono di fare ricerche da riga di comando e anche di mantenere sotto controllo l’infrastruttura. Queste rispettano una precisa sintassi. Esistono poi anche annotazioni che possono essere di vario genere e sono di libero utilizzo. 

E che succede al balancing? K8 è in grado di avvalersi di un sistema name resolver, utilissimo sia in caso di cambiamenti delle istanze tra le quali si effettua il bilanciamento sia nel caso in cui un master cada. 

Una funzione essenziale è quella di load balancing, che nella istanza più basilare è in grado di smistare richieste alternativamente, ma che in genere ha delle logiche anche più complesse per assegnare i task a una o l'altra istanza. 
Permette inoltre di avere delle istanze di backup che entrino in azione nel momento del bisogno. 
Questo tipo di infrastruttura, capace di gestire più nodi in differenti macchine fisiche o virtuali, permette quindi anche di ottenere un'infrastruttura distribuita in modo da permettere una zonizzazione, se inserita nelle regole di deploy per il master. Altrimenti la compatibilità con le risorse è la logica primaria con cui il master decide in quale nodo dovrà essere inserito il POD in fase di creazione. 

 Una prima implementazione che non ricade nel caso delle kubernetes, ma è illustrativa dei principi alla base del load balancing è quella che sfrutta[[8.3 Tool - Nginx as HTTP Load Balancer| Nginx come balancer HTTP]]. 

Per un uso in locale delle Kubernetes [[10.1 Tool - Minikube|Minikube]] si rivela uno strumento molto prezioso. 
### Load Balancing 

Service e Developer sono le due modalità di load balancing che possono essere usate da Kubernetes. 
Si possono avere load balancer attivi al:
- livello 4, quindi ognuno con la propria porta e vincolato al livello 4
- livello 7, che direzionano il traffico in base all’URI, sniffando le richieste grazie alla variabile host dell’header in cui è contenuto l’indirizzo del server cui si deve accedere. 

Il load balancing avanzato è un sistema simile ai totem delle poste: è in grado di indirizzare la richiesta in una coda ad un certo sportello tra molti ed è in grado di farlo in base al tipo di richiesta fatta. 

Rende automatizzabile il controllo dello stato del sistema con l’accesso e consultazione tramite specifici URI sulle nostre piattaforme. Basandosi sul POD, che identifica un sottoinsieme di container che girano su uno o più nodi, è in grado di agire secondo una logica di onere inverso e entrare in azione nel caso in cui non siano segni di vita da parte del servizio di interesse per un periodo definito. 

Ingress: componenti che a partire da un IP pubblico è in grado di fare da balancing a livello 7. 
Killercoda: soluzione da 60 minuti al giorno che permette di sperimentare con un cluster. 

Diamo ora una rapida occhiata alle componenti e i principi base in dettaglio. 

# Cluster
da [Kubernetes 101](https://medium.com/google-cloud/kubernetes-101-pods-nodes-containers-and-clusters-c1509e409e16)

Anche se lavorare con i singoli nodi può essere utile, non è la via di Kubernetes. In generale, dovresti pensare al cluster nel suo insieme, invece di preoccuparti dello stato dei singoli nodi.

In Kubernetes, i nodi uniscono le loro risorse per formare una macchina più potente. Quando distribuisci programmi sul cluster, gestisce in modo intelligente la distribuzione del lavoro ai singoli nodi per te. Se i nodi vengono aggiunti o rimossi, il cluster si sposterà intorno al lavoro se necessario. Non dovrebbe importare al programma, o al programmatore, quali singole macchine stanno effettivamente eseguendo il codice.

Se questo tipo di sistema simile a una mente alveare ti ricorda i Borg di Star Trek, non sei solo; "Borg" è il nome del progetto interno di Google su cui si basava Kubernetes.

## Kubeconfig

Tutto in Kubernetes è descritto tramite un file YAML, e lo è anche la descrizione del cluster: esiste un file, in effetti, che si chiama kubeconfig e che permette di descrivere i dettagli per accedere al cluster, come l’indirizzo a cui raggiungere le API, eventuali certificati e via dicendo. Questo file è memorizzato all’interno del path $HOME/.kube/config e solitamente ha un aspetto simile a questo:

```
apiVersion: v1clusters:
- cluster:    
    certificate-authority-data: xxx    
    server: https://kubernetes.docker.internal:6443  
  name: my-cluster
- contexts:
  context:    
    cluster: my-cluster    
    user: myuser  
  name: my-cluster
current-context: my-cluster
kind: Config 
preferences: {}
users:
- name: myuser  
  user:     
    token:
```

Le informazioni per avere accesso a un cluster sono quindi: 

- certificate-authority-data: l’Autorità Certificativa che ci garantisce l’accesso al cluster;
- server: l’endpoint del cluster (di uno dei nodi control-plane);
- name: nome del cluster;
- user: nome dell’utente che accede al cluster;
- token: token dell’account utente.

> Se ci si collega a più di un cluster in diverse occasioni, ognuno di questi potrebbe riportare le proprie informazioni all’interno del kubeconfig: non a caso, il campo clusters e contexts, così come users, sono liste, che permettono la definizione di più entità.

Se si ha a che fare con un cluster diverso da quello locale, spesso ci si deve collegare utilizzando delle altre metodologie: per esempio, è possibile configurare la variabile di ambiente KUBECONFIG del proprio terminale (o di quello di server a cui accediamo) con il percorso del file kubeconfig per la connessione al cluster. 
In questo modo, ovunque tu stia utilizzando il comando kubectl dal terminale, la variabile di ambiente KUBECONFIG dovrebbe essere disponibile. Impostando questa variabile, il contesto del cluster corrente viene sovrascritto e potrai collegarti al nuovo senza difficoltà. 
## Nodo master: il Control Plane

> I componenti che girano su questi nodi lavorano insieme per accettare le richieste degli utenti, determinare i modi migliori per pianificare la gestione dei container e del loro carico di lavoro, autenticare i client e i nodi worker, configurare la rete a livello di cluster e gestire il ridimensionamento del sistema.

Qui troviamo: 

### ETCD

Sviluppato dal team di CoreOS, è un sistema open source che lavora come archivio di coppie chiave-valore molto leggero che può essere configurato per estendersi su più nodi. 

Kubernetes utilizza etcd per archiviare i dati di configurazione a cui è possibile accedere da ciascuno dei nodi nel cluster; dal momento si tratta di un sistema distribuito, è fondamentale avere un database che supporti la persistenza di una configurazione condivisa, garantendo ai diversi componenti di mantenere traccia delle informazioni relative al loro stato e agli eventuali aggiornamenti delle informazioni a disposizione.

Se viene eseguito un aggiornamento su un nodo, come per esempio una nuova applicazione che viene eseguita, questo cambio di configurazione nell’architettura viene immediatamente riportata nel database di etcd, ancor prima che questo avvenga; in effetti, è proprio etcd a dare il via a qualsiasi cambiamento nel cluster, dal momento che si occupa di confermarne lo stato desiderato e l’avvio ai lavori soltanto quando le informazioni saranno persistite al suo interno. Oltre a questo, etcd aiuta anche a mantenere lo stato del cluster tramite funzionalità come l’elezione del leader, che sfrutta l’algoritmo di consenso Raft; questo consente di mantenere alta la disponibilità del cluster anche nel caso uno o più nodi falliscano.

> Memorizza tutte le configurazioni, gli stati e i metadati degli oggetti Kubernetes: che siano essi Pod, Secret, Daemonset o anche Deployment, ogni informazione relativa al loro stato viene riportata su questo database, così da poter “ricostruire” la situazione se ci sono guasti nei nodi worker. etcd memorizza tutti gli oggetti in una directory chiamata /registry tramite il formato valore-chiave. Per esempio, le informazioni su un Pod denominato Nginx all’interno nel namespace di default, saranno disponibili sotto la cartella /registry/Pods/default/nginx.

### API Server

> È l’hub centrale del cluster Kubernetes che espone le API per comunicare con Kubernetes. Sia gli utenti che gli altri componenti del cluster comunicano con esso tramite questo server.
>È responsabile di gestire le API ed esporre un endpoint come punto di accesso per gestire le richieste in ingresso, di autenticarle tramite certificati o token, di elaborarle e convalidarne l’output, e lo fa comunicando direttamente con etcd. Si occupa dunque di coordinare tutti i processi tra i nodi control-plane e quelli worker.

Ad esempio il comando kubectl dietro alle quinte sta comunicando con il server API tramite delle chiamate REST HTTP. 
Alcuni componenti interni del cluster come lo scheduler, il controller e così via comunicano con il server API utilizzando gRPC, un sistema che si occupa di fornire un processo di gestione delle chiamate per collegare tra loro più componenti. gRPC semplifica di molto la comunicazione tra due o più oggetti, fornendo un sistema di messaggistica tra componenti che possono comunicare in maniera sicura. La comunicazione tra il server API e altri componenti nel cluster avviene comunque tramite TLS per impedire l’accesso non autorizzato al cluster.

![[Sensini - Rapporto API Server e resto delle componenti.png]]

### kube-scheduler

> È responsabile della pianificazione dei Pod sui nodi worker: gestisce le richieste per la creazione di nuove risorse applicative ospitate all’interno di container all’interno di uno dei nodi del cluster, secondo le specifiche Risorse assegnate, come CPU, memoria, persistenza e tutta una serie di ulteriori componenti. 

È il componente che preso contezza delle risorse assegnate si occupa di di determinare quale POD possa gestirle correttamente. Deve quindi conoscere la capacità totale dei nodi e le risorse già allocate ai carichi di lavoro esistenti su ciascun server; queste informazioni può ottenerle grazie alla comunicazione con il server che mette a disposizione le API relative al cluster, ossia kube-api.

![[Sensini - Kube Scheduler e componenti in rapporto.png]]

### Kube Controller Manager

Nelle Kubernetes, i controller sono loop di controllo che osservano lo stato del tuo cluster, quindi apportano o richiedono modifiche dove necessario. Ogni controller tenta di riportare lo stato corrente del cluster più vicino possibile allo stato desiderato. Questo è quello che fa il kube controller manager per i POD che sono stati creati. 

> Il Kube controller manager è un componente che gestisce tutti i controller Kubernetes: come vedremo nei prossimi capitoli, le risorse e gli oggetti Kubernetes come Pod, namespaces, Jobs e molti altri sono gestiti dai rispettivi controller; e indovinate chi gestisce lo scheduler? Proprio il Kube controller manager.

![[Sensini - Kube controller manger e altre componenti.png]]

### Cloud Controller Manager

> Questo controller funge da ponte tra le API della piattaforma utilizzata e il cluster quando si fa il deploy di K8 sul cloud. 

Kubernetes può essere distribuito in molti ambienti diversi e può interagire con vari provider per comprendere e gestire lo stato delle risorse nel cluster. In questo modo i componenti principali di Kubernetes possono funzionare in modo indipendente e consentire ai fornitori di servizi cloud di integrarsi con Kubernetes utilizzando i relativi plugin. L’integrazione del controller consente al cluster Kubernetes di eseguire il provisioning di risorse cloud come istanze (per aggiungere dei nodi), Load Balancer (per la comunicazione di rete) e volumi di archiviazione (per i volumi persistenti) in maniera trasparente. Questo infatti è stato originariamente creato per consentire di sviluppare Kubernetes indipendentemente dall’implementazione dello specifico cloud provider. 

Funge da collante per consentire a Kubernetes di interagire con i provider e le relative funzionalità, mantenendo al contempo costrutti relativamente generici al proprio interno.

> Consente quindi a Kubernetes di aggiornare le informazioni sullo stato in base alle informazioni raccolte dal provider, di adattare le risorse cloud quando sono necessarie modifiche nel sistema e di creare e utilizzare servizi aggiuntivi per soddisfare i requisiti di lavoro inviati al cluster.

![[Sensini - Cloud Provider e Cloud Provider manager rispetto agli altri nel Control Plane.png]]

### CoreDNS

Il DNS è una delle funzionalità principali di Kubernetes: permette di lavorare con oggetti come i Services per esporre verso l’esterno le applicazioni; kube-dns o CoreDNS sono estensione indispensabile per il cluster K8s. 

Quando il Pod dovrà comunicare con altre risorse, all’interno e all’esterno del cluster, dovrà sapere dove inviare la richiesta. Con il sistema DNS, i Services in Kubernetes possono essere referenziati per nome che corrisponde ai Pod gestiti dal Service stesso, utilizzando un nome di dominio completo (chiamato anche FQDN, ossia Fully Qualified Domain Name), al posto dell’indirizzo IP (propio come il sistema di tag con [[8.4 Tool - Traefik|Traefick]]). 

CoreDNS può essere utilizzato per fornire un servizio di questo tipo ed è parte di Kubernetes dalla versione 1.13, al posto del precedente kube-dns, che è stato dismesso per avere una maggior efficienza e una minor occupazione delle risorse. Questo componente, essendo di vitale importanza, gira sui nodi control-plane.


![[Sensini - Core DNS.png]]


### CNI Plugin

Molti provider di rete hanno creato soluzioni basate su CNI per container con un’ampia gamma di funzionalità di rete. Kubernetes 1.26 supporta i plugin CNI (acronimo di Container Network Interface) per la gestione del networking dei cluster e sono disponibili diversi plugin (sia open source che closed source) nell’ecosistema Kubernetes. 

Ciò consente agli utenti di scegliere una soluzione di rete che meglio si adatta alle loro esigenze da diversi fornitori, dal momento che è necessario un plugin CNI per implementare il modello di rete Kubernetes.

> Il Kube-controller-manager è responsabile dell’assegnazione del [[3.7 Il Livello Rete in Internet#CIDR – classless interdomain routing|CIDR]] degli indirizzi IP relativi a ciascun Pod per ogni nodo. Ogni Pod riceve un indirizzo IP univoco a partire dal CIDR del Pod. 
> Kubelet interagisce con il container runtime per avviare il Pod e il plugin CRI che fa parte dell’ambiente di esecuzione del container interagisce con il plugin CNI per configurare la rete di Pod. Questo consente il collegamento in rete tra Pod distribuiti su nodi uguali o diversi utilizzando una rete sovrapposta. Alcuni plugin conosciuti includono Calico, Flannel, Weave Net, Cilium, Amazon VPC CNI, Azure CNI. 

> Il networking Kubernetes è un argomento importante e differisce in base alle piattaforme di hosting.
## Nodi Worker

I componenti che risiedono nei nodi worker permettono la corretta esecuzione dei carichi di lavoro. 
### Kubelet 

> È un agente che viene eseguito su ogni nodo worker del cluster come demone, gestito da systemd responsabile della corretta gestione delle attività dei container nel POD.

È responsabile di quanto segue: creazione, modifica ed eliminazione dei container per il Pod, gestione del loro avvio e della loro corretta esecuzione, dei relativi volumi e della raccolta dei dati dello stato del nodo e del Pod. 

Kubelet è anche un controller che verifica le modifiche relative ai Pod e utilizza l’ambiente di esecuzione dei container del nodo per effettuare il pull delle immagini e tutte le attività che riguardano l’applicazione.

Comunica con i nodi control-plane per autenticarsi con il cluster e ricevere le istruzioni per poter operare; la definizione delle risorse con cui deve operare avviene per mezzo di un file in formato YAML, che viene anche definito manifest, il quale definisce l’oggetto e i parametri operativi di ciò che deve creare e/o gestire. Il processo kubelet si assume quindi la responsabilità di mantenere lo stato delle risorse coerente a quanto descritto nel database di etcd.

### Container Runtime 

> Il container runtime è responsabile dell’avvio e della gestione dei container, delle applicazioni incapsulate in un ambiente operativo relativamente isolato ma leggero.

Ogni unità di lavoro sul cluster è implementata come uno o più container che devono essere distribuiti; il container runtime su ciascun nodo è il componente che esegue i container definiti nei carichi di lavoro inviati al cluster; viene eseguito su tutti i nodi nel cluster Kubernetes ed è responsabile del pull delle immagini dai registry come DockerHub o simili, dell’esecuzione dei container, dell’allocazione e dell’isolamento delle risorse per i container e della gestione dell’intero ciclo di vita di un container su un host. In questo dominio, esistono due concetti chiave.

- Il *Container Runtime Interface* (CRI) è quel componente che rappresenta un insieme di API per consentire a Kubernetes di interagire con diversi ambienti di esecuzione per container. In questo modo, è possibile utilizzare in modo intercambiabile diversi strumenti per container con Kubernetes, visto che definisce un’API per la creazione, l’avvio, l’arresto e l’eliminazione di container, nonché per la gestione di immagini e la loro comunicazione. Per fare un esempio, Docker Engine è un container runtime che rispetta gli standard definiti all’interno di CRI.
- L’*Open Container Initiative* (OCI) è invece un insieme di standard per i formati e i tempi di esecuzione dei container.

Kubernetes supporta più container runtime, e ciò significa che tutti questi implementano l’interfaccia CRI ed espongono le API per l’esecuzione e la gestione delle immagini.

![[Sensini - Container Runtime e il resto.png]]

### Kube Proxy

> Ogni container nel POD è eseguito come se fosse un host separato, ma abbiamo bisogno che comunichino tra loro in base alle specifiche: perché questo accada senza aumentarne la complessità abbiamo a disposizione il componente kube proxy. 

Su ciascun nodo worker, viene eseguito un piccolo e leggero servizio proxy. kube-proxy è infatti un demone (come kubelet) che implementa il concetto di Services per i Pod. 
Questo processo inoltra le richieste ai container giusti, può eseguire il bilanciamento del carico delle richieste di base ed è generalmente responsabile di assicurarsi che la rete sia correttamente configurata e accessibile, ma anche isolata se necessario. 


# Pod

Si tratta di un gruppo di container individuabile per funzioni omogenee (fatto che ci agevola per la scalarità orizzontale). È la più piccola unità funzionale nel contesto di K8. 

Per quel che riguarda la scalarità verticale nelle situazioni in cui non sia verificabile la prima, ad esempio il classico caso di unico db disponibile; ovvero aumentare le capacità della singola macchina. 

Il POD rappresenta uno o più container che devono essere gestiti come fossero una singola applicazione stateless: questo vuol dire che, se il container va in errore, questo può essere riavviato. I Pod sono costituiti da container che operano in stretta collaborazione, condividono un ciclo di vita e devono sempre essere programmati sullo stesso nodo. Sono gestiti interamente come un’unità e condividono il loro ambiente, i volumi e lo spazio IP.

> Più container all’interno dello stesso Pod condividono lo stesso spazio di rete e di conseguenza le stesse porte: i processi che sono in esecuzione in container all’interno dello stesso Pod devono evitare conflitti sulle porte utilizzate; tutti i container all’interno del Pod appartengono alla stessa interfaccia di rete, per cui un container può comunicare con gli altri tramite localhost.

Per quanto possa sembrare fuorviante e in contraddizione, la struttura del Pod lascia una porta aperta non solo al concetto di sidecar, ma a tutte quelle situazioni in cui un insieme processi strettamente correlati debbano lavorare insieme e avere (quasi) lo stesso ambiente, come se fossero tutti in esecuzione in un singolo container, pur mantenendoli in qualche modo isolati. In questo modo, si ottiene il meglio da entrambi i mondi: sfruttare tutte le funzionalità fornite dai container, dando allo stesso tempo ai processi l’illusione di funzionare insieme. Quando si tratta di filesystem, le cose sono leggermente diverse: per impostazione predefinita il filesystem di ciascun container è completamente isolato dagli altri; tuttavia, è possibile condividere directory e file utilizzando i [[#Volumi|volumi]].

Per riassumere per decidere se raggruppare dei container in un Pod, oppure in due Pod separati, è possibile porsi le seguenti domande:

- I container devono essere eseguiti insieme o possono essere eseguiti su diversi host?
- I container rappresentano un insieme unico o sono componenti indipendenti?
- I container contengono più processi con funzionalità diverse?
- I container devono essere ridimensionati insieme o singolarmente?

L’ultima domanda ci pone, davanti a un assunto importante: nel caso in cui si abbiano più container in un Pod, il ridimensionamento orizzontale è generalmente sconsigliato, perché si rischia di trattare allo stesso modo due container con risorse e necessità differenti, soprattutto dal momento che esistono altri oggetti di livello superiore più adatti all’attività. In altre parole, devi sempre valutare come prima opzione di separare i tuoi container in diversi Pod, a meno che un motivo specifico non richieda che facciano parte dello stesso Pod.

## File-pod.yml 

Il file che diamo in pasto a Kubernetes per gestire i pod. 
Qui sono indicati tutta una serie di parametri necessari al funzionamento. 

> Nota: in caso di dubbio per un controllo rapido e offline il comando `kubectl explain` include un piccolo sommario dei campi e delle possibili configurazioni. È possibile ottenere una lista di queste pagine di manuale grazie al comando `kubectl api-resources`.

- apiVersion: V1
Fatto interessante che ci siano dei range predefiniti di compatibilità. Gli strumenti garantiscono compatibilità tra versioni diverse a patto che si resti a meno di due versioni di distanza tra tool e cluster e che non si cerchi di usare caratteristiche più recenti su cluster vecchi. Il versioning è semantico, per cui il numero nel mezzo indica la version minore utilizzabile. Occorre assicurarsi di essere sempre a una distanza massima di tre versioni per essere certi del supporto. 

- Kind: Tipo di oggetto. Nel caso del POD, si scrive Pod. Esistono molti oggetti con cui possiamo avere a che fare e i campi che seguono, anche se simili nei nomi, assumono un significato diverso tra un’entità e l’altra; inoltre la tipologia permette a Kubernetes di validare l’oggetto con cui stiamo lavorando e scovare eventuali errori nella sintassi, sempre sfruttando le API.

- metadata: informazioni descrittive della risorsa, come il nome del Pod, eventuali label
- name: il nome del pod  

- spec: indica le specifiche. Si tratta di una serie di attributi riferiti alle funzionalità del Pod. Si tratta degli oggetti ospitati e tutto ciò che riguarda il ciclo di vita del Pod: possiamo elencare tutti i container che vogliamo il Pod includa all’interno della sezione containers, così come eventuali informazioni su una rete diversa, come gestire il riavvio del container, e altro ancora.
	Ad esempio la dimensione minima del Pod e della sua dimensione massima oltre la quale lo scheduler deve intervenire per spostarne il funzionamento, vitale per evitare malfunzionamento. Può darsi anche che si indichi quali nodi in cui inserire. Tipicamente la prima specifica che si inserisce è l'immagine da utilizzare, quindi il container. 
	- containers: qui possono essere più voci. Ognuna deve indicare le caratteristiche per ogni container. 
		- image: le immagini prese dalla repository, nome immagine e versione. 
			- name: nome del container
			- ports: le porte esposte dal container
				- -containerport: 8080
					- name: ad esempio http
					- protocol: ad esempio TCP

Il campo status è l’unico campo di cui non dovremo occuparci: questo viene compilato da Kubernetes e riporta lo stato attuale del Pod. Nella maggior parte dei casi, le persone che lavorano con queste risorse non devono modificarlo.

Ecco un file di esempio: 
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

Oppure: 
```
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
  - name: kuard
    image: grc.io/kuar-demo/kuard-amd64:blue
    ports:
      - containerPort: 8080
        name: http
        protocol: TCP 
```

Per poi dare il comando di creazione del pod si usa:
`kubectl apply -f file`

>Attenzione: si applica il medesimo discorso che per i file docker-compose: non si usa tab ma solo gli spazi per indentare il file. Inoltre, si ricordi che si può specificare il namespace di appartenenza proprio al memento in cui si da il comando di creare un pod. 

## Ciclo di vita

Il Pod è inoltre progettato come “mortale” o transiente: quando un Pod “muore”, come nel caso in cui questo avvenga a causa del controller Kubernetes che è costretto ad arrestarlo per risorse insufficienti, questo non potrà essere riavviato autonomamente. In questo caso, avremo bisogno di un oggetto di più alto livello che ci permetta di gestire lo stato dei Pod al posto nostro, ed è il caso dei controller.

Per ottenere lo stato del pod si usa il comando: 
`kubectl get pods`

Riguardo al ciclo di vita di un POD: lo stato di quello appena creato nell’esempio proposto è *running*, che chiaramente rappresenta che il Pod è in esecuzione correttamente. Questo valore definisce infatti una “fase” del ciclo di vita del Pod, che non ha un intervallo di tempo limitato, ma è la sua condizione momentanea. Altre fasi che il Pod può “vivere” durante il suo utilizzo all’interno di un cluster sono lo stato *pending*, ossia un intervallo di tempo nel quale il Pod è stato accettato da Kubernetes, ma il container e la relativa immagine non sono ancora state create. Un altro stato è *failed*, che chiaramente rappresenta la condizione in cui tutti i container presenti nel Pod sono stati arrestati, e almeno uno di essi ha riportato un errore, producendo un codice diverso da zero durante la terminazione.

### Che cosa succede, all’atto pratico, quando viene istanziato un Pod? 

Per prima cosa, questo avrà uno stato pending: sarà necessario infatti che lo scheduler trovi un nodo pronto a ospitare il Pod e kubelet si occuperà di scaricare l’immagine del container e di avviarlo. Questo rimarrà in stato pending fintanto che non saranno state predisposte queste informazioni, e poi potrà finalmente porre in stato running il Pod. Se il Pod venisse arrestato per qualche motivo, potrebbe non essere ricreato se non sono presenti dei controller (che vedremo in seguito) e quindi assumerebbe lo stato failed o succeeded, a seconda del codice di stato di uscita dei container: se il Pod viene arrestato e termina senza problemi, avrà come valore succeeded, mentre i Pod che si trovano in uno stato pending passano allo stato failed a causa, per esempio, di un errore di battitura nel nome dell’immagine o nella versione dell’immagine. Infine, se il Pod è nello stato unknown, significa che non è stato possibile ottenere lo stato del Pod, e questo solitamente quando c’è un problema di comunicazione con il nodo su cui questo dovrebbe risiedere o un problema con il cluster.

Il comando per ottenere i log di un pod è: 
`Kubectl logs nomepod`

## Sidecar Container

Si tratta di un pattern tipico, chiamato container secondario: il sidecar pattern è un tipo di modello in cui ci sono due container, dove il primo rappresenta il servizio applicativo, senza il quale l’applicazione non esisterebbe, mentre il secondo ha lo scopo di aggiungere valore al primo, condividendo rete e storage (Fonte: Designing Distributed Systems di Brendan Burns. O’Reilly Media, Inc. 2018).

Il caso d’uso più comune di questa tipologia di accoppiamento è il caso in cui si lavori con un web server che ha bisogno di memorizzare e gestire i log tramite un servizio apposito: il container principale conterrà quindi il server che espone l’applicazione, mentre il sidecar si occuperà del servizio relativo ai log.

Un altro esempio riguarda la sincronizzazione del codice sorgente dell’applicazione con un repository Git: in questo caso, il sidecar container si occupa di aggiornare il codice rispetto agli ultimi commit, al posto di utilizzare una pipeline apposita per questa attività; oppure, nel caso in cui si lavori con uno strumento come Nginx o Apache, è possibile che sia necessario modificare la configurazione di questi server per far sì che l’aggiornamento sia immediato. Utilizzare un sidecar container permette, per esempio, di leggere la configurazione del server aggiornata da un sito statico e ricaricare quella attuale del server se vengono rilevate delle modifiche. 

## Init Container

>È un tipo speciale di container che viene eseguito prima di quelli “applicativi”; questi devono essere eseguiti e terminati correttamente prima che si avvii l’esecuzione dei container dell’applicazione principale. 

Sono diversi dai container dell’applicazione per diverse ragioni: se il Pod ha molti Init Container, ciascuno di essi viene eseguito a partire dal completamento del precedente, finché non saranno esauriti e si potrà passare al container finale. 
Inoltre, se l’Init Container riportasse un errore, ci sono due strade:
- è possibile definire una policy per cui il Pod verrà riavviato fintanto che questo container “speciale” non avrà esito positivo 
- il Pod può essere segnato come failed.

Grazie al lavoro fatto con i controller, i Pod sono oggetti con cui difficilmente avremo a che fare direttamente: l’approccio più comune è quello che prevede la creazione di queste risorse e la relativa gestione tramite un Deployment (o altre tipologie di controller disponibili) per risparmiarci gran parte della fatica, e affidare a qualcuno la salvaguardia della nostra applicazione.

Gli Init Container sono considerate entità separate dai container delle applicazioni e usano immagini diverse con diverse configurazioni. Sono utili per molteplici scopi: vengono utilizzati per l’inizializzazione dell’applicazione, come può essere il recupero di dati necessari per la configurazione dell’applicazione, o anche per ritardare la creazione del Pod finché non vengano soddisfatte una serie di precondizioni, come la connessione a un servizio.

Possono contenere utility o codice ad hoc per l’installazione di risorse che non sono presenti in un’immagine dell’applicazione, senza che sia necessario avere un’immagine di base da utilizzare; è possibile sfruttare i campi command e args per utilizzare strumenti come sed, awk o Python ed eseguire una serie di controlli prima che il container applicativo parta.

Un altro esempio: un Deployment per Nginx che utilizzi un Init Container per poter creare una prima pagina da mostrare all’utente: per farlo, sfrutteremo un’immagine estremamente semplice, ossia quella di busybox, e tramite questa risorsa aggiungeremo un file index.html con una prima riga di HTML. Nel container principale, aggiungiamo invece il container vero e proprio: Nginx ospiterà la nostra pagina e la mostrerà agli utenti.

Per farlo, di seguito viene riportato un esempio: prima definiamo il Deployment chiamato nginx e in seguito il campo template.

```
apiVersion: apps/v1
kind: Deployment
metadata:  
  labels:    
    app: nginx-server  
  name: nginx
spec:  
  replicas: 1  
  selector:    
    matchLabels:      
    app: nginx-server  
template:    
  metadata:      
    labels:        
      app: nginx-server    
spec:      
  volumes:      
  - name: common-disk        
    emptyDir: {}      
  initContainers:      
  - name: busybox        
    image: busybox        
    volumeMounts:        
    - name: common-disk          
      mountPath: /web-files        
    command: ["/bin/sh"]        
    args: ["-c", "echo '<h2>Init container added this line!</h2>' > /web-files/index.html && sleep 20"]      
    containers:      
    - image: nginx        
      name: nginx        
      volumeMounts:        
      - name: common-disk          
        mountPath: /usr/share/nginx/html”
```

## Risorse per i Pod 

Sperimentando con [[1.4 Kuard|Kuard]] abbiamo sin dalla interfaccia online l'opzione di aumentare le risorse. Se lo facciamo oltre le risorse del nodo questo viene ucciso. Questo perché già di suo sono configurati dei limiti dovuti alla macchina in uso, ma questa non è la buona pratica.

Ogni pod è mantenuto in vita tramite un processo healthcheck che controlla che il tutto sia sempre in running, altrimenti lo scheduler lo riavvia in automatico. 

Requests e limits sono due meccanismi utilizzati da Kubernetes per controllare risorse come CPU e memoria: le prime servono a definire ciò che Kubernetes dovrà garantire al container per potersi avviare; se un container richiede una risorsa, Kubernetes la pianificherà solo su un nodo che può fornirgli tali risorse, che quindi rappresentano un valore minimo che permette all’applicazione di partire correttamente. I limits, d’altra parte, servono ad assicurarsi che un container non superi mai un certo valore: in altre parole, al container è consentito utilizzare le risorse solo fino al limite specificato, che è quindi limitato.

È vitale inserire dei limiti di risorse, minimi e massimi, per ogni pod per evitare che si verifichino condizioni di overflow di memoria e simili in caso di difficoltà per la macchina. 

Per farlo si utilizza al solito il file di configurazione.
Due saranno le sezioni di interesse, messe al livello di name: 
- resources: si tratta delle risorse minime da utilizzare 
- limits: come facile immaginare, le risorse massime. 

Per la CPU l'unità di misura sono i core, indicati in unità di misura delle KubernetesCPU. Normalmente si assegnano millesimi di core, per cui per un intero core il valore sarà 1000m.
La memoria RAM è misurata in Mi, ovvero bytes in base 10. 

```
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
  - name: kuard
    image: grc.io/kuar-demo/kuard-amd64:blue
    resources:
      cpu: "500m"
      memory: "128Mi"
    limits:
      cpu: "1000m"
      memory: "550Mi"
    ports:
      - containerPort: 8080
        name: http
        protocol: TCP 
```

Requests e limits sono definiti per container: sebbene i Pod di solito contengano un singolo container, abbiamo detto che è possibile incontrare anche Pod con più container. Ogni container nel Pod dovrà quindi avere il proprio limite e la propria richiesta individuale, tenendo presente che, poiché i Pod sono sempre eseguiti come un gruppo di container, è necessario sommare i limiti e le richieste per ogni container per ottenere un valore aggregato per il Pod.

Limiti e richieste di memoria sono misurati in byte e puoi esprimere il suo valore come numero intero semplice o come numero intero in virgola fissa utilizzando uno di questi suffissi: E, P, T, G, M, K. Puoi anche utilizzare gli equivalenti potenza di due: Ei, Pi, Ti , Gi, Mi, Ki. Il tutto nasce dal kibibyte: questo è stato progettato per sostituire il kilobyte in quei contesti informatici in cui il termine kilobyte è usato per indicare 1024 byte, e dove l’interpretazione di kilobyte per denotare 1024 byte era in conflitto con la definizione SI del prefisso kilo (1000).

Per calcolare correttamente questi valori, l’ideale è quello di eseguire dei test di carico attraverso strumenti come Apache JMeter e raccogliere tutte le metriche in base a delle specifiche funzionalità o transazioni e poi prendere la media, il massimo e il minimo della CPU e della memoria per estrapolare requests e limits.

A onor del vero, c’è anche un altro metodo: si tratta dell’utilizzo di una risorsa chiamata Vertical Pod Autoscaler, che è un componente che è possibile installare nel cluster e che serve a stimare le requests e limits per i diversi Pod.
### Healthcheck e health probes

Non appena un Pod viene “schedulato” (se ne pianifica l’esecuzione) su un nodo, sarà kubelet a eseguire su quel nodo i suoi container e, da quel momento in poi, li manterrà in esecuzione finché il Pod esiste. Se il processo principale del container si arresta in modo anomalo, kubelet riavvierà il container; se l’applicazione ha un bug che ne causa l’arresto di tanto in tanto, Kubernetes la riavvierà automaticamente.

Questi sono casi molto semplici, che tengono conto di quando le applicazioni smettono di funzionare senza che il loro processo principale si arresti in modo anomalo. Per esempio, un’app Java che abbia un memory leak (in estrema sintesi, un consumo di memoria non previsto perché non si libera lo spazio da ciò che non è utilizzato attivamente dal processo) inizierà a lanciare una raffica di OutOfMemoryErrors nei log, ma il processo principale della JVM continuerà a funzionare, per cui potremmo potenzialmente non accorgercene ma”

> Per questo, esistono le cosiddette probes: si tratta di test, di diversa natura, che permettono di controllare lo stato del container e di reagire in base alla situazione e ai parametri preimpostati in modo automatico.

 In Kubernetes ne esistono tre versioni, e ognuna di esse assume un significato diverso: liveness probe, readiness probe e startup probe.

Kubernetes mette a disposizione diversi meccanismi: il primo è quello di eseguire una richiesta HTTP GET al container e, se il codice di risposta è positivo, dichiarare il successo del test, altrimenti il container verrà riavviato. 
Un altro metodo è quello di sfruttare una socket, e quindi tentare l’apertura di una connessione TCP a una specifica porta del container: se la connessione avviene con successo, allora il test avrà esito positivo, altrimenti (indovina) il container verrà riavviato. 
L’ultimo metodo è quello di eseguire un comando che ci permetta di verificare lo stato del container, e che ritorni un codice pari a 0 se la risposta è affermativa, un numero qualsiasi altrimenti. 
Tutti i metodi elencati sono validi e non c’è una preferenza particolare: la scelta dipende molto dallo stack tecnologico con cui abbiamo a che fare, e che ci permette già di fare una prima scrematura; nel caso di un’applicazione web, sarà più semplice fare una richiesta a un path specifico per ottenere una risposta, mentre con alcuni tipi di servizi dovremo lavorare con un comando personalizzato e adatto al caso d’uso.
#### Liveness Probe 

La presenza di un pod tra i processi non è sufficiente a determinare se questo è a tutti gli effetti funzionanti. Un processo in deadlock comunque risulterebbe ancora in esecuzione ma non sarebbe funzionale per esempio alle richieste API. 
	Le sonde all'interno sono presenti solo se sono state programmate nelle immagini in uso. Di questo non ci occupiamo direttamente e dobbiamo lavorare in contatto con i developer delle nostre applicazioni. 

K8 ha introdotto anche le liveness e readiness healthcheck: riesce a riconoscere delle logiche stabilite che lo sviluppatore deve avere integrato, ovvero aver creato l'endpoint nell'applicazione per raccolta di dati che vadano oltre la semplice presenza. Questi devono poi essere indicati nel file manifest, in cui può anche essere indicato il timing di queste richieste e anche il numero di volte che sono necessarie ad innescare un'azione dello scheduler.
Ogni container nel pod ha un controllo dedicato. 

Ad esempio sono utili i liveness probe e le r. Verifica se alla richiesta della pagina si ottiene una risposta tra 200 e 400 (ecco perché è fondamentale che il developer le abbia integrate correttamente), andando a battere un percorso healthy con un codice di risposta. In altri casi potrebbe anche trattarsi di altri tipi di richiesta. 
	Un tipo di liveness probe è infatti quello HTTP-GET, prevedendo che per un certo path (un URI) a una data porta dia una certa risposta, per cui viene anche indicato un tempo di inizio per la sonda, tempo che ha inizio successivamente allo startup probe. Si deve anche indicare il tempo di timeout oltre la quale non si attende più risposta, e il periodsecond, ovvero il tempo che si deve attendere prima di eseguire nuovamente il test. Inoltre, esiste anche il parametro failuretreshold oltre il quale si considera il container non funzionante. 
Le azioni fatte in base alla risposta sono definite in base alla restart policy inserita nel manifest, con tre differenti possibilità: always, on-failure (se fallisce il test delle sonde o con uscita con exitcode 0) oppure never. 

Ogni controllo ha un ritardo di tempo di partenza, un numero massimo di fallimenti, il comportamento da fare in caso di fallimento. 
#### Readiness Probe

La readiness probe invece riguarda la prontezza a seguire le richieste utente. I container che falliscono questo test sono rimossi dal service load balancer. Fondamentale in fase di debug perché possiamo andare ad interagire col container senza che sia distrutto dallo scheduler e non soltanto leggerne i log postumi.

>Ovviamente le risposte devono essere significative rispetto alla probe in questione, con endpoint differenti. 

Le richieste HTTP per le liveness sono incoming traffic, per cui costituisce anche una readiness probe a tutti gli effetti. Infatti, in genere si usano altro genere di probes, come ad esempio dei comandi di base che prevedano una risposta per le liveness. 

#### Startup Probe

Un altro controllo viene eseguito dopo lo startup, per quanto ad esso inerente, dopo un certo tempo. Quando si fa un esperimento il punto interessante non è mai quello in transizione: interessa il risultato del valore definitivo, e qui è lo stesso. È previsto che dopo un certo periodo di startup il pod sia pronto, ma che non si facciano gli stessi controlli prima che questa fase sia stata superata, altrimenti le risposte potrebbero non aver senso e scatenare azioni impreviste nello scheduler. 

In genere si usa per soluzioni di particolare complessità composte da molti elementi differenti e con tempistiche differenti o spesso interdipendenti e di conseguenza tempi di avvio più lunghi, usato per controllare che nel deploy si stiano facendo tutti i passaggi necessari.  
### Errori Comuni con le Probes

Ci sono diversi errori che vengono commessi di frequente. 

Il primo è il non definire alcun tipo di health check per il nostro applicativo, il quale non verrà mai riavviato in caso di problemi e rimarrà sempre all’interno del pool di loadbalancing di un Service.

Il secondo tipo di errore riguarda il definire liveness e readiness probes uguali, ad esempio andando a contattare lo stesso identico endpoint HTTP. Questo può essere dovuto ad una comprensione sbagliata di questi tipi di test. La liveness probe è collegata al concetto di applicazione in salute, quindi nel caso fallisca, verrà riavviato il Pod. La readiness probe è relativa all’inserimento o meno del Pod nel pool di loadbalancing durante tutto il ciclo di vita del Pod. Definirle uguali potrebbe far rischiare ad esempio il riavvio di un applicativo che è temporaneamente lento per via di molto traffico. 
In breve: è corretto prevedere le due logiche funzionali: una permette di monitorare la funzionalità è l'altra la disponibilità dei servizi. 

Un altro aspetto importante, è il non far fallire le probes nel caso una delle dipendenze del servizio in esame sia down. Questo infatti sarebbe solamente il propagare il problema, che invece deve essere rilevato nel servizio opportuno.

Un esempio in cui una liveness probe potrebbe non essere necessaria è quando, per esempio abbiamo, a che fare con un server che espone dei file statici: l’applicazione si avvierà velocemente e uscirà solamente nel caso in cui riscontri un errore che gli impedisca di mostrare le pagine. Dal momento che il processo di Nginx produce un codice di errore ogni volta riscontra un problema di configurazione o di esposizione delle pagine, il container sarà riavviato naturalmente come avverrebbe per qualunque altra situazione analoga. Al contrario, la readiness probe è necessaria: questo perché il server andrà a gestire del traffico di rete in entrata e in uscita e, ogni volta che abbiamo a che fare con questa tipologia di applicazioni, un test del genere ci permette di evitare problemi all’avvio del container o magari nel momento in cui dovessimo aumentare le risorse a disposizione del cluster con un flusso di lavoro che cambia. Per fare un ultimo paragone, nel caso in cui avessimo a che fare con un job che si occupa di compiere delle attività ripetitive o programmate, sarà certamente utile a configurare una liveness probe per verificare che il processo sia in esecuzione correttamente e che, nel caso in cui il container riscontrasse un problema, possa essere gestito in maniera opportuna, ma non sarà necessario configurare una readiness probe: abbiamo detto che questa serve a mandare un segnale a Kubernetes per comunicare che il container è pronto a gestire del traffico di rete e che quindi la comunicazione con l’esterno e con altre applicazioni può essere attivata.
### Altri errori comuni in K8
Come i paragrafi precedenti, segue da: 
[I 5 errori più comuni con Kuberentes](https://italiancoders.it/i-5-errori-piu-comuni-dei-principianti-con-kubernetes/)
#### Il selector delle labels su un service non ha un match con i pod

Per poter funzionare correttamente come bilanciatore di rete, un Service specifica in genere dei selectors, i quali permettono di trovare i Pod che fanno parte del pool di bilanciamento. Nel caso non ci sia un match, il Service non avrà Endpoints ai quali inoltrare il traffico. Teniamo presente che il loadbalancing verso i Pod è di tipo casuale .

#### Porta sbagliata del container mappata sul service

Ogni Service ha due parametri fondamentali “_targetPort_” e “_port_” i quali vengono spesso confusi e usati in modo scorretto. Questo risulta quindi in messaggi di errore come “Connection Refused” o la mancanza di risposta ad una richiesta.

In modo da evitare questo errore, ricordiamo che “_targetPort_” è la porta destinazione nei Pod, quella a cui un Service va ad inoltrare il traffico. Il parametro “_port_” invece è riferito alla porta esposta dal Service verso i client.

Esse possono essere uguali ovviamente, importante è conoscerne il significato!

####  Risorse – requests e limits

In genere i principianti dimenticano di settare requests e limits per CPU e memoria. Questo errore porta ad avere dei nodi worker sui quali sono schedulati troppi applicativi, diventando “overcommitted”. Il risultato finale sono applicativi poco performanti.

Altre volte invece vengono settati dei limits troppo bassi, che portano nel caso della CPU a basse performance e nel caso della memoria al “kill” del Pod. Se hai già visto il messaggio di errore OOMkill si tratta proprio di questo problema. La strategia ideale è assegnare la giusta quantità di CPU e memoria, possibilmente settando le requests pari ai limits (Guaranteed QoS) per applicazioni critiche.

Il modo per capire le risorse necessarie è sostanzialmente effettuare dei test e monitorare con tools come Prometheus o simili.

#### Troppi services di tipo loadbalancer	

Capita spesso che i principianti espongano troppi Service all’esterno utilizzando il tipo Loadbalancer. In questo caso, un controller specifico, fornito dal cloud provider, andrà a creare un Loadbalancer esterno al cluster, all’interno della VPC. Queste risorse possono diventare abbastanza costose, soprattutto se sono associate ad indirizzi IP statici.

La soluzione migliore è esporre il cluster con un singolo Loadbalancer che andrà ad inoltrare il traffico ai nodi del cluster, i quali in genere hanno un ingress controller per effettuare il routing utilizzando risorse di tipo Ingress.

#### Eseguire comandi senza specificare un namespace

Durante i workshop ho notato che in genere gli studenti dimenticano di posizionarsi sul namespace corretto, o comunque dimenticano il flag -n per specificare un namespace. È importante essere sicuri di utilizzare o specificare il namespace corretto per eseguire un comando!

## Variabili di ambiente

Come per [[8. Docker|Docker]], è possibile utilizzare dei file per indicare in maniera dichiarativa delle variabili di ambiente che modifichino i comportamenti dei container. 

Perché sono così utili?
Inserire dei file con i dati scolpiti nell’immagine è simile alla configurazione hardcoded nel codice sorgente dell’applicazione, perché richiede di ricostruire l’immagine ogni volta che si desidera modificare la configurazione, e non rientra nelle buone pratiche. Inoltre, chiunque abbia accesso all’immagine può vedere la configurazione, incluse tutte le informazioni che devono essere mantenute segrete, come credenziali o chiavi di crittografia.

Una premessa è doverosa: le risorse che vedremo adesso sono perfette quando abbiamo a che fare con dati che possono cambiare a seconda del contesto, come le opzioni della JVM (per chi sviluppa in Java), oppure opzioni che cambiano a seconda dell’ambiente, come i dati di connessione a un database: questo non vuol dire che le variabili di ambiente o i comandi aggiuntivi sono banditi, ma piuttosto che abbiamo la possibilità di rendere più modulare l’applicazione che installeremo su Kubernetes.

Due sono le tipologie di file utilizzate:
- secrets: si tratta di file utili proprio per casi in cui si scolpiscono dati sensibili nelle pagine, come nel caso del file [[[1. Laboratorio - Form di accesso|login.php]].
- file di configurazione classico. 

## ConfigMaps

Kubernetes consente di separare le opzioni di configurazione in un oggetto separato chiamato ConfigMap, che è di base una mappa (o un dizionario, a seconda della scuola di programmazione da cui vieni) contenente coppie chiave/valore con valori che possono essere sia letterali brevi sia file di configurazione completi. Questo tipo di oggetto è perfetto per memorizzare informazioni generiche di configurazione e non “sensibili”, come file di properties, configurazioni applicative o argomenti utili ai processi in esecuzione sul container.

L’applicazione non ha bisogno di leggere direttamente la ConfigMap o addirittura sapere che esiste, perché il contenuto di questa risorsa viene invece passato ai container come variabili di ambiente o come file tramite un volume; poiché è anche possibile fare riferimento alle variabili di ambiente negli argomenti della riga di comando utilizzando la sintassi $(VARIABILE_AMBIENTE), è anche possibile passare i valori della ConfigMap ai processi come argomenti della riga di comando.

È possibile crearle in modi diversi.

### File diretto

È possibile scrivere un file di configurazione con tutti i campi compilati. Dopo averlo fatto non si farà altro che dare il comando `kubectl create -f myconfigmap.yml`.

```
kind: ConfigMap
apiVersion: v1
metadata:  
  name: myconfigmap-files
data:  
  locale.properties: |-  
# ###################################################################      
# Locale properties compliant with the standard Java i18n mechanism        
#  # see https://docs.oracle.com/javase/8/docs/api/java/util/Locale.html      # for extensive explanations       
#  
# the file provides default empty values for each property         
# ###################################################################          language = @locale.properties.language@       
country = @locale.properties.country@         
variant = @locale.properties.variant@”
```

### Pescando da un file di configurazione 

È possibile anche crearne una a partire da un file di configurazione già presente o da riga di comando: nel primo caso, possiamo per esempio prendere il file di configurazione riportato e utilizzare kubectl create configmap come comando per creare questa risorsa, come mostrato di seguito:

```
“# see "man logrotate" for details
# rotate log files weeklyweekly
# use the adm group by default, since this is the owning group
# of /var/log/syslog.su root adm# keep 4 weeks worth of backlogsrotate 4
# create new (empty) log files after rotating old 
# use date as a suffix of the rotated file
#dateext
# uncomment this if you want your log files compressed
#compress
# packages drop log rotation information into this directoryinclude /etc/logrotate.d
# system-specific logs may be also be configured here.
```

Il comando poi è `“kubectl create configmap myconfigmap-logrotate --from-file=logrotate.conf”`

Potrebbe sembrare che questa risorsa sia più che altro un modo per strutturare bene il nostro progetto, ma che non porti alcun vantaggio significativo: in realtà, esternalizzando la configurazione in questo modo, abbiamo la possibilità di modificare qualsiasi valore contenuto al suo interno anche quando il Pod è già in esecuzione, senza doverlo riavviare manualmente. Se prendessimo l’esempio precedente, e andassimo a modificare la ConfigMap tramite il comando `kubectl edit`, dopo qualche istante, noteremmo che il file è stato aggiornato correttamente.

## Secret

> I Secret sono molto simili alle ConfigMap: sono anche loro mappe che contengono delle coppie chiave-valore e possono essere utilizzati nello stesso modo di una ConfigMap, quindi è possibile passare i valori del Secret al container come variabili di ambiente o esporli come file in un volume. 

I Secret, che ci permettono di salvare credenziali, certificati, password e altre informazioni che non devono essere condivise.

Al contrario delle ConfigMap, Kubernetes aiuta a proteggere i Secret assicurandosi che ciascuno di essi sia distribuito solo nei nodi che eseguono i Pod e che necessitano dell’accesso al Secret. Inoltre, sui nodi stessi, i Secret sono sempre archiviati in memoria e mai scritti su un archivio fisico; infatti, a partire dalla versione 1.7 di Kubernetes, etcd archivia i Secret in forma crittografata, rendendo il sistema molto più sicuro.

Segue un esempio di secret. 

```
“kind: Secret
apiVersion: v1
metadata:  
  name: mysecret
data:  
  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...
type: Opaque
```

I contenuti delle chiavi del campo data di un Secret sono mostrati come stringhe codificate in Base64, mentre quelli di una ConfigMap sono mostrati in chiaro. Può sembrare complesso ma questa scelta deriva dal fatto che si possano memorizzare all’interno dei Secret anche dati binari: i dati che memorizziamo possono infatti contenere molti caratteri non riconosciuti, e codificarli con un formato così noto è conveniente. Questo non è dunque un modo per renderli più “sicuri”, in quanto con Base64 si sta codificando, e non crittografando: questo consente di conservare le informazioni in modo più semplice.
	Nel creare una stringa, è tuttavia possibile inserire i dati utilizzando un formato semplice: questo può avvenire sostituendo, al posto del campo data, il campo stringData

Il campo stringData è di sola scrittura, e questo vuol dire che può essere utilizzato solo per impostare i valori. Quando si recupera lo YAML del Secret con il comando kubectl get secret, il campo stringData non verrà mostrato. Invece, tutte le voci specificate nel campo stringData (come la voce foo nell’esempio precedente) verranno mostrate sotto data e saranno codificate in Base64 come tutte le altre voci.
## Volumi

Si tratta di delle aree in cui è possibile depositare informazioni in modo permanente e cui i container possono far riferimento, proprio come per [[8. Docker|Docker]].

Si indica nella sezione spec del file di configurazione quali siano i volumi cui si possa far accesso in base a quel che è elencato nel manifest. Qui è indicata la cartella di origine, ma all'interno di questa possono essere definite molte. Interessante che non tutti i container sono tenuti a montare i volumi, perciò è possibile che si indichi un container specifico soltanto in cui interagire, quindi che vada a interagire solamente con una sottocartella di cui ho mappato il path nelle impostazioni del container specifico. 

```
[...]
spec:
  volumes:
    - name: "kuard-data"
      hostPath:
        path: "/Users/stefanotosetto/Kubernetes/Kuard/Dati"
    - name: "kuard-conf"
      hostPath:
        path: "/Users/stefanotosetto/Kubernetes/Kuard/Info"
  containers:
    -image: grc...
    name: kuard
    volumeMounts:
     - name: "kaurd-data"
       mountPath: "/data"
```

Questo rende possibile puntare più container ad una stessa risorsa, o comunque contenere più volumi in una serie di sottocartelle nella macchina host. 

Altra nota interessante, quel path non necessariamente coincide con una risorsa locale: potrebbe trattarsi di una repository online, come GithHub, o persino servizi come Google Drive e simili. 

Esistono vari tipi di volumi, che si distinguono in persistenti e non, e che sono pensati per scopi specifici. 

Ad esempio {emptydir}, un volume di durata pari a quella del container alcuni non sono persistenti e sono usati per far comunicare container differenti. È una sorta di cartella di condivisione che viene usata all'interno del pod. Con aperta e chiusa parentesi non ha un limite in spazio, mentre con sizelimit si può prendere il controllo sulle sue dimensioni. 
	Esistono container funzionali al corretto funzionamento del principale, detti sidecar container. Potrebbero ad esempio fare un echo all'interno del principale per poi spegnersi. In questo modo due container che non condividono mai il filesystem possono interagire a livello di filesystem. Emptydir punterà nella cartella in cui deve scrivere nel principale nelle specifiche del container principale e invece al punto in cui il container scaricherà o scriverà nel suo filesystem.

Un file di esempio:

```
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      sizeLimit: 500Mi
```

Un esempio pratico possiamo ritrovarlo 

Ben altro tipo di volume è l'hostPath, che è un volume persistente (dunque che sopravvive all'arresto anomalo o distruzione del container). Si tratta di un qualcosa che è proprio di ogni singolo nodo: per questo non è buona idea usarla per database ad esempio: se il pod cui fa riferimento dovesse essere spostato di nodo sarebbe tutto perso. Usato più per applicazioni di sistema come fluentd, usato per recuperare i logs, oppure per sistemi di pod a container singolo come avveniva per Docker. 

Using the hostPath volume type presents many security risks. If you can avoid using a hostPath volume, you should. For example, define a local PersistentVolume, and use that instead.

If you are restricting access to specific directories on the node using admission-time validation, that restriction is only effective when you additionally require that any mounts of that hostPath volume are read only. If you allow a read-write mount of any host path by an untrusted Pod, the containers in that Pod may be able to subvert the read-write host mount.
Take care when using hostPath volumes, whether these are mounted as read-only or as read-write, because:
- Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs (such as the container runtime socket), that can be used for container escape or to attack other parts of the cluster.
- Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to different files on the nodes.
- hostPath volume usage is not treated as ephemeral storage usage. You need to monitor the disk usage by yourself because excessive hostPath disk usage will lead to disk pressure on the node.
Some uses for a hostPath are:
- running a container that needs access to node-level system components (such as a container that transfers system logs to a central location, accessing those logs using a read-only mount of /var/log)
- making a configuration file stored on the host system available read-only to a static pod; unlike normal Pods, static Pods cannot access ConfigMaps

Altra tipo di volume persistente è GitRepo. Si tratta di un volume molto utile in cui ci si basi su file condivisi che debbano essere costantemente aggiornati. Si tratta di una directory vuota che al suo interno contiene una copia dei file presenti nella repository cui punta. È stato però di recente deprecato e si suggerisce di ottenere lo stesso risultato con una cartella emptyDir. 

## Replicas-set

Si tratta di una componente che nel suo manifest riporta come e quanto replicare dei pod. È fondamento della scalarità orizzontale.

Nel pod devono essere contenuti unicamente elementi di logica uniforme perché l'infrastruttura possa funzionare. 

Vediamo ora un esempio di manifest. 

```
apiVersion: apps/v1
kind: ReplicaSet
 name: kuard-replica
 labels:
   app: my-app-kuard
spec: 
  replicas: 3
  selector:
    matchLabels:
      app: my-app-kuard
    template: 
      metadata:
        labels:
          app: my-app-kuard
    spec:
      containers: 
        - name: kuard
          image: grc.io/kuar-demo/kuard-amd64:blue
          - containerPort: 80
```

Il sistema andrà a cercare ogni container o pod che abbia come label "my-app-kuard" e ne creerà tre repliche, che saranno gestite da questo sistema. Se se ne crea uno a mano con lo stesso label verrà inserito nel replicaset. Oppure, se ne si cancella uno, immediatamente un altro gli succede.
	Se ne desume che si possa scalare unicamente dal file di configurazione o da console con il comando apposito, ovvero, `kuebctl scale replicaset nome --replicas=x`. Esiste inoltre la possibilità di utilizzare un comando che permette di modificare il manifest senza dover fare una cancellazione e un apply, ovvero il comando edit. 
Sotto al numero stabilito non si va. Sopra al numero stabilito non si va. 

Tipicamente si può scalare anche da riga di comando, ma è opportuno sempre ritornare al manifest e modificarlo in modo che non ci si trovi nello stesso caso visto con Docker-Compose: che lo stesso comando dia un risultato diverso. 

Questo strumento in configurazione avanzata la capacità di gestire operazioni di aggiornamento basate sulla logica DevOps. Come developer si può definire una modalità più graduale per passare da una versione all'altra. Inoltre conserva la vecchia versione in base a limiti stabiliti, per cui è capaci di fare un roll-back. 

Replicaset espone anche i log per singolo container creato, che avrà per nome la radice indicata e un progressivo. 

Volendo si può eliminare un replicaset senza annichilire i pod. Se si ricrea il replicaset subito dopo, si ripartirà a gestire le repliche secondo il numero inserito nel replicaset. 

Interessante che oltre che stabilire sidecar contiainer si possono avere anche degli initcontainer, ovvero dei container che inizializzino elementi per altri container si possono stabilire le priorità di avvio grazie a più fattori. Uno di questi è elencare questi container come initcontainers. 
# Database

Molti sono i cms e software che si basano su di un DBMS. Ma stanziare un’immagine con un suo db interno è un problema per le logiche stesse messe in campo nel caso dei container. Occorre inserire una serie di comandi che creino e popolino le tabelle. Ma questo farebbe si di dover far comunicare i container e l’uno eseguire operazioni sull’altro, cosa possibile, ma non pratica per questa soluzione. 

Vediamo ora un caso in cui costruiamo un pod in cui avremo un container che opera da dbms e un init container che creerà una tabella utenti con id e nome utente. Il container init si avvia, si collega al db, crea db e tabella e aggiunge dati. 

Il db è in grado di instanziarsi in base a un file configmap in una specifica directory. 
Quindi è possibile far depositare dall’initcontainer in una directory condivisa con il container db e mappata alla directory corretta. 
L’invito potrebbe anche andare a pescare il file in una repository esterna il file da rendere disponibile a MySQL prima del suo avvio perché ne usi le specifiche. Sono i cosiddetti ConfigMaps. 

# Services 

Si tratta dell’elenco di container presenti in un replicas set. Gli oggetti sono tra loro isolati, possono comunicare o tramite gli strumenti messi a disposizione da Kubernetes tramite il serve API. 

Questi sono elencati nel file pod. Da qui elenchiamo anche le porte e altri elementi fondamentali. Possiamo anche forzare la presenza o meno di un certo container con gli initicontainer. 
Definiamo anche ambiente minimo e massimo in cui lo scherduler lascia che questi operi. 

Definiamo anche delle cartelle, che si distinguono in persistenti o transitorie e che possono essere identificate con dei punti di mount specifici e che possono essere specchiate rispetto ad dei configmap, che possono essere poi identificati per ogni container. 
	Il configmap altro non è che un documento con dei valori. Potrebbe anche essere un contenitore di file, ovvero descrivere dei file e in questo modo ottenere che sia mappato dal container. Potrei avere diverse configurazioni con le stesse immagini ad esempio, e questo permette di instanziare più container in vario modo. Ad esempio potrei avere una password per la fase di sviluppo e una più complessa per la fase di produzione. 
	Le variabili di ambiente ci permettono di esternalizzare delle configurazioni rispetto al container, che hanno effetto per ogni instanziazione. Si tratta di un ulteriore livello di astrazione della configurazione. 

Il manifest del pod tipicamente contiene tutte queste informazioni. Ma le variabili di ambiente possono anche essere ulteriormente contenute in un file differente. Questo possono essere anche molto sensibili rispetto al funzionamento. Per questo alcuni configmap possono essere dei secrets, perché contengono preziosissimi dati per proteggerne l’accesso. I secrets sono file che Kubernetes tratta in modo speciale, rendendoli accessibili in modo differente rispetto agli altri e trattandoli con maggior cura.

Cluster IP, Non, load balancer, external/ingress: si tratta delle opzioni con cui è possibile gestire la connessione dei singoli container nel sistema K8. Possiamo in questo modo dedicare degli indirizzi ip a determinati servizi o meno. È possibile poi inserire un port forward per i servizi in deployment che ne hanno bisogno. 

Un POD può aver una classe indirizzamenti interna per i suo indirizzamenti interni. Il POD non può raggiungere gli altri servizi, a meno che non si sia stabilito diversamente. Potrebbe essere ad esempio che con dei label si sia inserito in una certa classe che identifichi come in deploy un servizio. 

# Deployments

# Networking

# I vantaggi e (sopratutto) gli svantaggi 
Da [Aruba Magazine:K8 la guida completa].(https://www.aruba.it/magazine/cloud/kubernetes-cose-come-funziona-la-guida-completa.aspx) e [Swarm and K8 comparison](https://phoenixnap.com/blog/kubernetes-vs-docker-swarm) 

L'obiettivo principale di Kubernetes è automatizzare la distribuzione, l'aggiornamento, la rete e la gestione del ciclo di vita dei sistemi containerizzati. La piattaforma controlla anche la stabilità dei sistemi e se funzionano in modo efficiente. Ad esempio, può riavviare, arrestare e riportare le applicazioni alle iterazioni precedenti se iniziano a influenzare negativamente altre applicazioni.   
  
La **scalabilità automatica di Kubernetes** alloca i carichi di lavoro delle applicazioni tra i nodi che compongono il cluster per ottimizzare il consumo di risorse. Ad esempio, se il traffico per un container è troppo elevato, la piattaforma può ridistribuire il carico per mantenere stabile la distribuzione. Ciò consente ai team IT di regolare manualmente la CPU, la memoria e lo spazio di archiviazione dei pod e ridurre i costi. Kubernetes vanta una vivace comunità di oltre 3.000 collaboratori e un ecosistema di software open source in crescita. Invece di racchiudere il proprio sistema operativo, i contenitori vengono eseguiti su un sistema operativo condiviso. Laddove un'applicazione containerizzata occupa solo megabyte, la stessa applicazione su una VM può raggiungere diversi gigabyte. Ciò rende i contenitori Kubernetes più veloci e ne consente il porting in qualsiasi ambiente.  
## Vantaggi

- Il set di funzionalità più completo sul mercato (individuazione del servizio, scalabilità orizzontale, self-healing, rollout e rollback automatizzati, esecuzione batch, monitoraggio integrato, ecc.).
- Ideale per la gestione di architetture di grandi dimensioni, carichi di lavoro cloud complessi e app avanzate basate su microservizi.
- Lo strumento funziona senza problemi su ogni sistema operativo.
- Lo strumento ha un set unificato di API e forti garanzie sullo stato del cluster.
- Una vasta gamma di funzioni di orchestrazione e automazione.
- Una serie di integrazioni e strumenti di terze parti.
- Tutti i principali fornitori supportano lo strumento.
- Una grande comunità attiva che spedisce continuamente nuove funzionalità e integrazioni.
- Pieno supporto della CNCF (Cloud Native Computing Foundation).
- Il tuo team può facilmente trovare documentazione e supporto per qualsiasi problema incontra.
## Svantaggi

- Un processo di installazione complesso
- Una curva di apprendimento ripida rende i K8 meno che ideali per i nuovi arrivati all'orchestrazione dei container.
- L'utente deve installare e sentirsi a proprio agio con strumenti CLI separati.
- In alcuni casi d'uso che coinvolgono app semplici, Kubernetes può portare a complicazioni inutili.
- Gli aggiornamenti frequenti richiedono un'attenta patch per evitare interruzioni o creare vulnerabilità.
- Troppo pesante per i singoli sviluppatori o anche per alcune piccole squadre.
- I team hanno spesso bisogno di strumenti aggiuntivi per gestire l'accesso e la governance.

Non si può dire che ogni azienda trarrà vantaggio dalla containerizzazione dei propri servizi. Diamo un'occhiata più approfondita agli svantaggi:  

### **Mancanza di strumenti integrati**  

Kubernetes non viene fornito con middleware, framework di elaborazione dati, database o sistemi di archiviazione cluster. Ma, come accennato, esiste un ecosistema in crescita di strumenti di terze parti, software open source e API per migliorarne le capacità. Questo è però a tutti gli effetti un potenziale vantaggio in un’ottica maggiormente aperta. 
  
###  **Problemi di visibilità**  

Monitorare tutto il traffico nel cluster di host è impegnativo, soprattutto se distribuito in diversi ambienti. Questo è probabilmente il motivo per cui il networking e il data logging posso essere sfide da considerare nell’uso di Kubernetes. Per fortuna, è possibile integrare un software di monitoraggio per migliorare la visibilità dell’ambiente.  
### **Problemi di sicurezza**  

Secondo un sondaggio RedHat condotto su 300 utenti di Kubernetes, circa il 59% ha considerato la sicurezza informatica come preoccupazione più significativa per la containerizzazione, con il 55% che subisce ritardi a causa di problemi di sicurezza. Firewall, strumenti di autenticazione e controllo degli accessi forniti dalle piattaforme informatiche non copriranno tutte le vulnerabilità e gli exploit zero-day. Tuttavia, i sistemi di prevenzione delle intrusioni (IPS) e sistemi di rilevamento delle intrusioni (IDS) per Kubernetes possono migliorare la sicurezza della rete.  
### **Dipendenza dalle risorse**  

L'adozione di Kubernetes richiede solitamente che le aziende facciano un investimento significativo nell'architettura dei microservizi, perché è necessario isolare le applicazioni prima di inserirle in contenitori. È inoltre necessario allocare risorse e spazio di archiviazione per ciascun microservizio, il che può aumentare i costi e complicare la gestione. Pertanto, potrebbe essere eccessivo (e proibitivamente costoso) pensare di adottare Kubernetes per le piccole imprese.  
  
Questo ci porta a una conclusione: sebbene il cluster Kubernetes su un’**architettura di microservizi** possa avvantaggiare le aziende in crescita, le imprese più piccole potrebbero sentirsi più a loro agio iniziando con un’architettura monolitica. Ma come evitare il costoso refactoring quando si è pronti ad adottare un'architettura di microservizi scalabile e liberamente accoppiata con contenitori? La risposta è adottare un approccio monolitico modulare. Ciò significa stabilire limiti forti per i moduli software così da ridurre al minimo le dipendenze. Quindi, man mano che l’azienda cresce, si troverà più semplice disaccoppiare questi moduli in microservizi indipendenti e containerizzarli.